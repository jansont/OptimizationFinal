{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm, eig\n",
    "from functools import partial\n",
    "from algorithms import conjugate_gradient, secant, Finite_Difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def armijo(x,\n",
    "#            cost_function,\n",
    "#            gradient,\n",
    "#            search_dir,\n",
    "#            gamma=1.5,\n",
    "#            r = 0.8,\n",
    "#            log=True):\n",
    "    \n",
    "#     def v_bar(cost_x,grad_x_s,w):\n",
    "#         return cost_x + 0.5*w*grad_x_s\n",
    "\n",
    "#     w = 1\n",
    "#     cost_x = cost_function(x)\n",
    "#     grad_x_s = np.dot(gradient,search_dir)\n",
    "#     # initialize p\n",
    "#     p = 0\n",
    "#     # propogate forward\n",
    "#     while cost_function(x + (gamma**p)*search_dir) < v_bar(cost_x, grad_x_s, gamma**p): \n",
    "#         w = gamma**p\n",
    "#         # increment p\n",
    "#         p += 1\n",
    "#     # initialize q\n",
    "#     q = 0\n",
    "#     # propogate backwards\n",
    "#     while cost_function(x + (r**q * gamma**p)*search_dir) > v_bar(cost_x, grad_x_s, r**q * gamma**p): \n",
    "#         # increment q\n",
    "#         q += 1\n",
    "#         # consider step size w\n",
    "#         w = r**q * gamma**p\n",
    "\n",
    "#     # return step size\n",
    "#     if log:\n",
    "#         print(f'p={p}, q={q}, w={w}')\n",
    "#     return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def steepest_descent1(x0,\n",
    "                     cost_function,\n",
    "                     gradient_function = None,\n",
    "                     step_size = 'armijo',\n",
    "                     threshold = 1e-4, \n",
    "                     log = False, \n",
    "                     h = 1e-8, \n",
    "                     max_iter = 1e6, \n",
    "                     gamma = 1.5, \n",
    "                     r = 0.8, \n",
    "                     fd_method = 'central', \n",
    "                     track_history = False):\n",
    "\n",
    "    #if no gradient function available, use finite difference appx\n",
    "    if gradient_function == None: \n",
    "        fd = Finite_Difference(cost_function, fd_method, h)\n",
    "        gradient_function = fd.estimate_gradient\n",
    "\n",
    "    x_history, V_history = [],[]\n",
    "    #initialize iterator, x, and gradient\n",
    "    i = 0\n",
    "    x = x0\n",
    "    gradient = gradient_function(x)\n",
    "    minimum = cost_function(x)\n",
    "\n",
    "    #iterate until near zero gradient or max iterations reached\n",
    "    while norm(gradient) >= threshold and i <= max_iter: \n",
    "\n",
    "        #update gradient\n",
    "        gradient = gradient_function(x)\n",
    "\n",
    "        #determine step size\n",
    "        if step_size == 'armijo':\n",
    "            step_size = armijo(x, cost_function, gradient, gamma, r, log = False)\n",
    "        elif isinstance(step_size, (int, float)):\n",
    "            pass\n",
    "        else: \n",
    "            raise ValueError('step size should be float, int or \"armijo\"')\n",
    "        \n",
    "        # move to a new x by moving from the original x in the negative\n",
    "        # direction of the gradient according to a given step size\n",
    "        x = x - step_size*gradient\n",
    "        minimum = cost_function(x)\n",
    "\n",
    "        #result tracking\n",
    "        i += 1\n",
    "        if log and i % 1e4 == 0: \n",
    "            print(f'x = {x}, V(x) = {minimum:.5f}')\n",
    "        if track_history: \n",
    "            x_history.append(x), V_history.append(minimum)\n",
    "\n",
    "    if track_history:\n",
    "        return x, minimum, x_history, V_history\n",
    "    else: \n",
    "        return x, minimum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo(x, cost, grad, s, gamma=1.5, mu=0.8):\n",
    "    w = 1  # Default step size\n",
    "\n",
    "    k_g = 0  # Power of gamma\n",
    "    k_m = 0  # Power of mu\n",
    "\n",
    "    # Precompute cost and gradient to save time\n",
    "    vx = cost(x)\n",
    "    gx_s = np.dot(grad(x), s)\n",
    "\n",
    "    def v_bar(w):\n",
    "        return vx + 0.5 * w * gx_s\n",
    "\n",
    "    while p.cost(x + gamma**k_g * s) < v_bar(gamma**k_g):\n",
    "        k_g += 1\n",
    "        w = gamma**k_g\n",
    "\n",
    "    while p.cost(x + mu**k_m * gamma**k_g * s) > v_bar(mu**k_m * gamma**k_g):\n",
    "        k_m += 1\n",
    "        w = mu**k_m * gamma**k_g\n",
    "\n",
    "    return w\n",
    "\n",
    "def steepest_descent(p, x, tol=1e-6, max_iter=999, hist=False):\n",
    "    i = 0\n",
    "    x_hist = []\n",
    "    while np.linalg.norm(p.grad(x)) > tol:\n",
    "        if i > max_iter:\n",
    "            break\n",
    "        s = -p.grad(x).T\n",
    "        w = _step_size(p, x, s)\n",
    "        x_hist.append(x)\n",
    "        x = x + w * s\n",
    "        i += 1\n",
    "\n",
    "    return x if not hist else np.array(x_hist)\n",
    "\n",
    "def _fd_grad(f, x, h=1e-8):\n",
    "    dim = np.max(np.shape(x))\n",
    "    grad_gen = ((f(x + h * np.eye(dim)[:, [i]]) - f(x)) / h\n",
    "               for i in range(0, dim))\n",
    "    grad = np.expand_dims(np.fromiter(grad_gen, np.float64), axis=0)\n",
    "    return grad\n",
    "\n",
    "\n",
    "class Problem:\n",
    "    \"\"\"Optimization problem\"\"\"\n",
    "    def __init__(self, cost, grad=None, grad_step=None,\n",
    "                 eq_const=None, ineq_const=None):\n",
    "        self._cost = cost\n",
    "        # Check presence of gradient function\n",
    "        if grad is None and grad_step is None:\n",
    "            # No gradient function, default step size\n",
    "            self._grad_step = 1e-8\n",
    "            self._grad = partial(_fd_grad, self.cost, h=self._grad_step)\n",
    "        elif grad is None and grad_step is not None:\n",
    "            # No gradient function, specified step size\n",
    "            self._grad_step = grad_step\n",
    "            self._grad = partial(_fd_grad, self.cost, h=self._grad_step)\n",
    "        elif grad is not None:\n",
    "            # Gradient given, no need for step size\n",
    "            self._grad_step = None\n",
    "            self._grad = grad\n",
    "        # Check presence of constraints\n",
    "        if eq_const is not None:\n",
    "            self._eq_const = eq_const\n",
    "        else:\n",
    "            self._eq_const = None\n",
    "        if ineq_const is not None:\n",
    "            self._ineq_const = ineq_const\n",
    "        else:\n",
    "            self._ineq_const = None\n",
    "\n",
    "    def cost(self, x=None):\n",
    "        if x is not None:\n",
    "            return self._cost(x)\n",
    "        else:\n",
    "            return self._cost\n",
    "\n",
    "    def grad(self, x=None):\n",
    "        if x is not None:\n",
    "            return self._grad(x)\n",
    "        else:\n",
    "            return self._grad\n",
    "\n",
    "    def eq_const(self, x=None):\n",
    "        if self._eq_const is not None:\n",
    "            if x is not None:\n",
    "                return np.array([[eq(x)] for eq in self._eq_const])\n",
    "            else:\n",
    "                return np.array([eq for eq in self._eq_const])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def ineq_const(self, x=None):\n",
    "        if self._ineq_const is not None:\n",
    "            if x is not None:\n",
    "                return np.array([[ineq(x)] for ineq in self._ineq_const])\n",
    "            else:\n",
    "                return np.array([ineq for ineq in self._ineq_const])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def num_eq_const(self):\n",
    "        \"\"\"Returns number of equality constraints\"\"\"\n",
    "        if self._eq_const is not None:\n",
    "            return np.max(np.shape(self._eq_const))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def num_ineq_const(self):\n",
    "        \"\"\"Returns number of inequality constraints\"\"\"\n",
    "        if self._ineq_const is not None:\n",
    "            return np.max(np.shape(self._ineq_const))\n",
    "        else:\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def augmented_lagrange(x0,\n",
    "                        cost_function, \n",
    "                        gradient_function,\n",
    "                        equality_constraints, \n",
    "                        inequality_constraints,\n",
    "                        tol=1e-6,\n",
    "                        tol_const=1e-6, \n",
    "                        sigma_max=1e12,\n",
    "                        hist=False):\n",
    "\n",
    "    def phi(x):\n",
    "        \"\"\"Unconstrained problem\"\"\"\n",
    "        cost = p.cost(x)\n",
    "\n",
    "        n_e = p.num_eq_const()\n",
    "        n_i = p.num_ineq_const()\n",
    "        n_c = n_e + n_i\n",
    "\n",
    "        lmb_e = lmb[0:n_e, :]\n",
    "        lmb_i = lmb[n_e:n_c, :]\n",
    "        sgm_e = sgm[0:n_e, :]\n",
    "        sgm_i = sgm[n_e:n_c, :]\n",
    "\n",
    "\n",
    "        if p.eq_const() is not None:\n",
    "            c_e = p.eq_const(x)\n",
    "            cost = cost - sum(lmb_e * c_e) + 0.5 * sum(sgm_e * c_e**2)\n",
    "        if p.ineq_const() is not None:\n",
    "            c_i = p.ineq_const(x)\n",
    "            p_i = np.array([-lmb_i[i] * c_i[i] + 0.5 * sgm_i[i] * c_i[i]**2 \n",
    "                            if c_i[i] <= lmb_i[i] / sgm_i[i] \n",
    "                            else -0.5 * lmb_i[i]**2 / sgm_i[i] \n",
    "                            for i in range(0, n_i)])\n",
    "            cost = cost + sum(p_i)\n",
    "        return cost\n",
    "\n",
    "    p = Problem(cost_function,\n",
    "                gradient_function,\n",
    "                eq_const=equality_constraints,\n",
    "                ineq_const=inequality_constraints)\n",
    "\n",
    "    x_hist = []\n",
    "\n",
    "    n_e = p.num_eq_const()\n",
    "    n_i = p.num_ineq_const()\n",
    "    n_c = n_e + n_i\n",
    "\n",
    "    lmb = np.zeros((n_c, 1))\n",
    "    sgm = np.ones((n_c, 1))\n",
    "\n",
    "    x = x0\n",
    "    c = 1e12 * np.ones((n_c, 1))\n",
    "\n",
    "    while np.linalg.norm(c) > tol_const:\n",
    "        # Create new problem to solve, but unconstrained\n",
    "        \n",
    "        up = Problem(partial(phi))\n",
    "\n",
    "        print(phi(x))\n",
    "\n",
    "\n",
    "        x_hist.append(x)\n",
    "        x = steepest_descent(up, x0, tol=tol)\n",
    "        x,_ = steepest_descent1(x0,\n",
    "                                phi,\n",
    "                                gradient_function = None,\n",
    "                                max_iter = 999,\n",
    "                                step_size = 'armijo')\n",
    "\n",
    "\n",
    "        # Concatenate costs\n",
    "        c_prv = c\n",
    "        c_e = p.eq_const(x)\n",
    "        c_i = p.ineq_const(x)\n",
    "        if c_e is not None and c_i is not None:\n",
    "            c = np.concatenate((c_e, c_i), axis=0)\n",
    "        elif c_e is not None:\n",
    "            c = c_e\n",
    "        elif c_i is not None:\n",
    "            c = c_i\n",
    "\n",
    "        # Make sure sigma is not too big\n",
    "        if any(sgm >= sigma_max):\n",
    "            break\n",
    "\n",
    "        # Update sigma\n",
    "        if np.linalg.norm(c, np.inf) > 0.25 * np.linalg.norm(c_prv, np.inf):\n",
    "            for i in range(0, n_c):\n",
    "                if np.abs(c[i]) > 0.25 * np.linalg.norm(c_prv, np.inf):\n",
    "                    sgm[i] *= 10\n",
    "            continue\n",
    "\n",
    "        lmb = lmb - (sgm * c)\n",
    "    print(x)\n",
    "    return x if not hist else np.array(x_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = lambda x: -x[0, 0] * x[1, 0]\n",
    "h1 = lambda x: -x[0, 0] - x[1, 0]**2 + 1\n",
    "h2 = lambda x: x[0, 0] + x[1, 0]\n",
    "\n",
    "\n",
    "def test_aug_lag():\n",
    "    x0 = np.array([[10], [1]])\n",
    "    x = augmented_lagrange(x0,\n",
    "                            cost_function = v,\n",
    "                            gradient_function = None, \n",
    "                            equality_constraints = None, \n",
    "                            inequality_constraints = [h1,h2], \n",
    "                            tol=1e-4\n",
    "                            ,tol_const=1e-4)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000004?line=0'>1</a>\u001b[0m test_aug_lag()\n",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb Cell 5'\u001b[0m in \u001b[0;36mtest_aug_lag\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000003?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_aug_lag\u001b[39m():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000003?line=6'>7</a>\u001b[0m     x0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m10\u001b[39m], [\u001b[39m1\u001b[39m]])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000003?line=7'>8</a>\u001b[0m     x \u001b[39m=\u001b[39m augmented_lagrange(x0,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000003?line=8'>9</a>\u001b[0m                             cost_function \u001b[39m=\u001b[39;49m v,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000003?line=9'>10</a>\u001b[0m                             gradient_function \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000003?line=10'>11</a>\u001b[0m                             equality_constraints \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000003?line=11'>12</a>\u001b[0m                             inequality_constraints \u001b[39m=\u001b[39;49m [h1,h2], \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000003?line=12'>13</a>\u001b[0m                             tol\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000003?line=13'>14</a>\u001b[0m                             ,tol_const\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000003?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb Cell 4'\u001b[0m in \u001b[0;36maugmented_lagrange\u001b[0;34m(x0, cost_function, gradient_function, equality_constraints, inequality_constraints, tol, tol_const, sigma_max, hist)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000002?line=61'>62</a>\u001b[0m x_hist\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000002?line=62'>63</a>\u001b[0m x \u001b[39m=\u001b[39m steepest_descent(up, x0, tol\u001b[39m=\u001b[39mtol)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000002?line=63'>64</a>\u001b[0m x,_ \u001b[39m=\u001b[39m steepest_descent1(x0,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000002?line=64'>65</a>\u001b[0m                         phi,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000002?line=65'>66</a>\u001b[0m                         gradient_function \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000002?line=66'>67</a>\u001b[0m                         max_iter \u001b[39m=\u001b[39;49m \u001b[39m999\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000002?line=67'>68</a>\u001b[0m                         step_size \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39marmijo\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000002?line=70'>71</a>\u001b[0m \u001b[39m# Concatenate costs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000002?line=71'>72</a>\u001b[0m c_prv \u001b[39m=\u001b[39m c\n",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb Cell 2'\u001b[0m in \u001b[0;36msteepest_descent1\u001b[0;34m(x0, cost_function, gradient_function, step_size, threshold, log, h, max_iter, gamma, r, fd_method, track_history)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000006?line=74'>75</a>\u001b[0m \u001b[39m#determine step size\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000006?line=75'>76</a>\u001b[0m \u001b[39mif\u001b[39;00m step_size \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39marmijo\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000006?line=76'>77</a>\u001b[0m     step_size \u001b[39m=\u001b[39m armijo(x, cost_function, gradient, gamma, r, log \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000006?line=77'>78</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(step_size, (\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/test2.ipynb#ch0000006?line=78'>79</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/ECSE507Final/OptimizationFinal/algorithms.py:419\u001b[0m, in \u001b[0;36marmijo\u001b[0;34m(x, cost_function, gradient, search_dir, gamma, r, log)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/algorithms.py?line=416'>417</a>\u001b[0m p \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/algorithms.py?line=417'>418</a>\u001b[0m \u001b[39m# propogate forward\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/algorithms.py?line=418'>419</a>\u001b[0m \u001b[39mwhile\u001b[39;00m cost_function(x \u001b[39m+\u001b[39m (gamma\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mp)\u001b[39m*\u001b[39msearch_dir) \u001b[39m<\u001b[39m v_bar(cost_x, grad_x_s, gamma\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mp): \n\u001b[1;32m    <a href='file:///Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/algorithms.py?line=419'>420</a>\u001b[0m     w \u001b[39m=\u001b[39m gamma\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mp\n\u001b[1;32m    <a href='file:///Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/algorithms.py?line=420'>421</a>\u001b[0m     \u001b[39m# increment p\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "test_aug_lag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e730323216fec8a00495a18d5e26a9184315a136cb5e6ee6f99d5a78808c1dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

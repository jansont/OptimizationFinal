{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from algorithms import conjugate_gradient, secant, Finite_Difference, armijo\n",
    "from cost_functions import V_a, gradV_a, V_b, gradV_b\n",
    "from numpy.linalg import norm, eig\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(x0,\n",
    "                     cost_function,\n",
    "                     gradient_function = None,\n",
    "                     step_size = 'armijo',\n",
    "                     threshold = 1e-4, \n",
    "                     log = False, \n",
    "                     h = 1e-8, \n",
    "                     max_iter = 1e6, \n",
    "                     gamma = 1.5, \n",
    "                     r = 0.8, \n",
    "                     fd_method = 'central', \n",
    "                     track_history = False):\n",
    "    '''\n",
    "    Performs vanilla gradient descent. \n",
    "    Args: \n",
    "        x0 :: np.array\n",
    "            Initial point of minization. Shape (n,)\n",
    "        cost_function :: Python function\n",
    "            Cost function to minimize. Rn -> R. \n",
    "        gradient_function :: Python function, or None\n",
    "            Gradient of cost_function. Rn -> Rn\n",
    "            If None, finite difference estimation of gradient is used. \n",
    "        step_size :: float or String\n",
    "            Step size to use during gradient descent. \n",
    "            If 'armijo', Armijo step size selection is used. \n",
    "            Default: 'armijo\n",
    "        threshold :: float\n",
    "            Threshold at which to stop minimization. Values \n",
    "            should be close to 0. Default: 1e-4\n",
    "        log :: bool\n",
    "            True to log optimization progress. Default: False\n",
    "        h :: float\n",
    "            Parameter for finite difference estimation. \n",
    "            Default 1e-8\n",
    "        max_iter :: int\n",
    "            Maximum optimization iterations. Default: 1e6\n",
    "        gamma :: float\n",
    "            Gamma parameter for armijo. Default is 1.5. \n",
    "        r :: float\n",
    "            r parameter for armijo. Default is 0.8. \n",
    "        fd_method :: string\n",
    "            Method for finite difference estimation. \n",
    "            Options: 'central', 'forward'\n",
    "        track_history :: bool\n",
    "            True to track points visited and corresponding cost. \n",
    "    Returns: \n",
    "        x :: np.array\n",
    "            Point at which minimization is reached. Shape (n,)\n",
    "        minimum :: float\n",
    "            Value of cost function at optimizer. \n",
    "        x_history :: list\n",
    "            List of points visisted. (if track_history = True)\n",
    "        V_history :: list\n",
    "            List of costs visisted. (if track_history = True)\n",
    "    '''\n",
    "\n",
    "    #if no gradient function available, use finite difference appx\n",
    "    if gradient_function == None: \n",
    "        fd = Finite_Difference(cost_function, fd_method, h)\n",
    "        gradient_function = fd.estimate_gradient\n",
    "\n",
    "    x_history, V_history = [],[]\n",
    "    #initialize iterator, x, and gradient\n",
    "    i = 0\n",
    "    x = x0\n",
    "    gradient = gradient_function(x)\n",
    "    minimum = cost_function(x)\n",
    "\n",
    "    #iterate until near zero gradient or max iterations reached\n",
    "    while norm(gradient) >= threshold and i <= max_iter: \n",
    "\n",
    "        #update gradient\n",
    "        gradient = gradient_function(x)\n",
    "\n",
    "        #determine step size\n",
    "        if step_size == 'armijo':\n",
    "            step_size = armijo(x, cost_function, gradient, gamma, r, log = False)\n",
    "        elif isinstance(step_size, (int, float)):\n",
    "            pass\n",
    "        else: \n",
    "            raise ValueError('step size should be float, int or \"armijo\"')\n",
    "        \n",
    "        # move to a new x by moving from the original x in the negative\n",
    "        # direction of the gradient according to a given step size\n",
    "        x = x - step_size*gradient\n",
    "        minimum = cost_function(x)\n",
    "\n",
    "        #result tracking\n",
    "        i += 1\n",
    "        if log and i % 1e4 == 0: \n",
    "            print(f'x = {x}, V(x) = {minimum:.5f}')\n",
    "        if track_history: \n",
    "            x_history.append(x), V_history.append(minimum)\n",
    "\n",
    "    if track_history:\n",
    "        return x, minimum, x_history, V_history\n",
    "    else: \n",
    "        return x, minimum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_lagrangian(x0, \n",
    "                         cost_function, \n",
    "                         gradient_function, \n",
    "                         equality_constraints = None, \n",
    "                         inequality_constraints = None,\n",
    "                         threshold = 1e-8, \n",
    "                         sigma_max = 1e8):\n",
    "\n",
    "    def phi(x):\n",
    "        cost = cost_function(x)\n",
    "        lambda_eq = lambd[:num_ec , :]\n",
    "        lambda_ineq = lambd[num_ec:num_c , :]\n",
    "        sigma_eq = sigma[:num_ec , :]\n",
    "        sigma_ineq = sigma[num_ec:num_c , :]\n",
    "    \n",
    "        for constraint in equality_constraints:\n",
    "            ec = constraint(x)\n",
    "            cost -= lambda_eq * ec + 0.5 * sigma_eq * ec **2\n",
    "        for i,constraint in enumerate(inequality_constraints):\n",
    "            ic = constraint(x)\n",
    "            if ic <= sigma_ineq[i] / sigma_ineq[i]: \n",
    "                t = -lambda_ineq[i] * ic + 0.5 * sigma_ineq[i] * ic**2 \n",
    "            else:\n",
    "                t = -0.5 * lambda_ineq[i]**2 / sigma_ineq[i] \n",
    "            cost += t\n",
    "        return(cost)\n",
    "\n",
    "    x_history, V_history = [],[]\n",
    "    num_ec = len(equality_constraints)\n",
    "    num_ic = len(inequality_constraints)\n",
    "    num_c = num_ec + num_ic\n",
    "\n",
    "    lambd = np.zeros((num_c,1))\n",
    "    sigma = np.ones((num_c,1))\n",
    "\n",
    "    cost = 1e12 * sigma\n",
    "    x = x0\n",
    "\n",
    "    while norm(cost) > threshold: \n",
    "\n",
    "        print(phi(x))\n",
    "\n",
    "        x,_ = steepest_descent(x, phi, None, step_size = 1e-4, threshold = 1e-3,)\n",
    "        previous_cost = cost\n",
    "        inequality_cost = [cost(x) for cost in equality_constraints]\n",
    "        equality_cost = [cost(x) for cost in inequality_constraints]\n",
    "        cost = np.array(inequality_cost + equality_cost).reshape((-1,1))\n",
    "\n",
    "        if any(sigma >= sigma_max):\n",
    "            break\n",
    "        if norm(cost, np.inf) > 0.25 * norm(previous_cost, np.inf):\n",
    "            for i in range(num_c):\n",
    "                if np.abs(cost[i]) > 0.25 * norm(previous_cost, np.inf):\n",
    "                    sigma[i] *= 10\n",
    "            continue\n",
    "        lambd = lambd - (sigma * cost)\n",
    "    minimum = cost_function(x)\n",
    "    return x, minimum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.]\n",
      "[-1.25049956]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000002?line=7'>8</a>\u001b[0m gradient_function \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000002?line=8'>9</a>\u001b[0m x0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m10\u001b[39m], [\u001b[39m1\u001b[39m]])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000002?line=10'>11</a>\u001b[0m x, minimum \u001b[39m=\u001b[39m augmented_lagrangian(x0,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000002?line=11'>12</a>\u001b[0m                     cost_function, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000002?line=12'>13</a>\u001b[0m                     gradient_function,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000002?line=13'>14</a>\u001b[0m                     equality_constraints, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000002?line=14'>15</a>\u001b[0m                     inequality_constraints)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000002?line=15'>16</a>\u001b[0m x, minimum\n",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb Cell 3'\u001b[0m in \u001b[0;36maugmented_lagrangian\u001b[0;34m(x0, cost_function, gradient_function, equality_constraints, inequality_constraints, threshold, sigma_max)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000001?line=38'>39</a>\u001b[0m \u001b[39mwhile\u001b[39;00m norm(cost) \u001b[39m>\u001b[39m threshold: \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000001?line=40'>41</a>\u001b[0m     \u001b[39mprint\u001b[39m(phi(x))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000001?line=42'>43</a>\u001b[0m     x,_ \u001b[39m=\u001b[39m steepest_descent(x, phi, \u001b[39mNone\u001b[39;49;00m, step_size \u001b[39m=\u001b[39;49m \u001b[39m1e-4\u001b[39;49m, threshold \u001b[39m=\u001b[39;49m \u001b[39m1e-3\u001b[39;49m,)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000001?line=43'>44</a>\u001b[0m     previous_cost \u001b[39m=\u001b[39m cost\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000001?line=44'>45</a>\u001b[0m     inequality_cost \u001b[39m=\u001b[39m [cost(x) \u001b[39mfor\u001b[39;00m cost \u001b[39min\u001b[39;00m equality_constraints]\n",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb Cell 2'\u001b[0m in \u001b[0;36msteepest_descent\u001b[0;34m(x0, cost_function, gradient_function, step_size, threshold, log, h, max_iter, gamma, r, fd_method, track_history)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000005?line=82'>83</a>\u001b[0m \u001b[39m# move to a new x by moving from the original x in the negative\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000005?line=83'>84</a>\u001b[0m \u001b[39m# direction of the gradient according to a given step size\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000005?line=84'>85</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m-\u001b[39m step_size\u001b[39m*\u001b[39mgradient\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000005?line=85'>86</a>\u001b[0m minimum \u001b[39m=\u001b[39m cost_function(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000005?line=87'>88</a>\u001b[0m \u001b[39m#result tracking\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000005?line=88'>89</a>\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb Cell 3'\u001b[0m in \u001b[0;36maugmented_lagrangian.<locals>.phi\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000001?line=22'>23</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000001?line=23'>24</a>\u001b[0m         t \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m lambda_ineq[i]\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m/\u001b[39m sigma_ineq[i] \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000001?line=24'>25</a>\u001b[0m     cost \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m t\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alixdanglejan-chatillon/Code/ECSE507Final/OptimizationFinal/al.ipynb#ch0000001?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m(cost)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "v = lambda x: -x[0, 0] * x[1, 0]\n",
    "h1 = lambda x: -x[0, 0] - x[1, 0]**2 + 1\n",
    "h2 = lambda x: x[0, 0] + x[1, 0]\n",
    "\n",
    "cost_function = v\n",
    "inequality_constraints = [h1, h2]\n",
    "equality_constraints = []\n",
    "gradient_function = None\n",
    "x0 = np.array([[10], [1]])\n",
    "\n",
    "x, minimum = augmented_lagrangian(x0,\n",
    "                    cost_function, \n",
    "                    gradient_function,\n",
    "                    equality_constraints, \n",
    "                    inequality_constraints)\n",
    "x, minimum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in []:\n",
    "    print(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e730323216fec8a00495a18d5e26a9184315a136cb5e6ee6f99d5a78808c1dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
